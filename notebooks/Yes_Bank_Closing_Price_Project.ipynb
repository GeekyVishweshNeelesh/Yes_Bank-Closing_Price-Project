{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Predictive Analytics for Yes Bank: Forecasting Monthly Closing Price with Machine Learning\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Supervised Learning and Regression\n",
        "##### **Contribution**    - Individual Project\n",
        "##### **Team Member 1 - Vishwesh Neelesh**"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Project Summary: Predicting Monthly Closing Stock Prices of Yes Bank Using Machine Learning\n",
        "\n",
        "This project aimed to design and implement a machine learning (ML) model capable of predicting the monthly closing stock prices of Yes Bank, leveraging historical stock market data. The primary goal was to derive a data-driven solution that can support financial forecasting, aid investment decision-making, and ultimately deliver meaningful business value.\n",
        "\n",
        "We began with a structured Exploratory Data Analysis (EDA) to understand the distribution, variability, and relationships among the available features. The dataset included key variables such as Date, Open, High, Low, Close, and Volume. Initial steps involved checking for null values, duplicate entries, and inconsistencies in the data. Missing value imputation techniques were applied where necessary to clean the data. Outliers were treated using IQR-based filtering, and numeric conversions were enforced to ensure proper modeling compatibility.\n",
        "\n",
        "A vital part of the project was feature engineering, where we introduced new variables like High_Low_Diff, Range_Pct, Price_Change, and Close_Open_Ratio. These enhanced features helped the model better capture stock volatility and trading behavior across different time periods. Feature correlation analysis and variable selection techniques were employed to retain only the most informative attributes and prevent overfitting.\n",
        "\n",
        "Three machine learning models were then developed: Linear Regression, Random Forest Regressor, and XGBoost Regressor. Each model was trained using the same input features and evaluated using standard regression performance metrics—Mean Squared Error (MSE), Mean Absolute Error (MAE), R² Score, and Adjusted R² Score.\n",
        "\n",
        "Linear Regression provided a baseline with excellent performance (R² ≈ 0.987), indicating strong linear relationships within the dataset. However, to capture more complex and non-linear interactions among variables, we implemented Random Forest and XGBoost models. Random Forest, an ensemble of decision trees, performed robustly with an R² of 0.977 and lower errors than Linear Regression after tuning.\n",
        "\n",
        "The best results were obtained from the XGBoost Regressor, particularly after applying hyperparameter optimization using GridSearchCV. The final tuned model achieved an R² score of 0.9766, with an MSE of 211.26 and MAE of 9.45. These results confirm the model’s ability to accurately predict monthly closing prices, generalize well to unseen data, and adapt to the non-linear nature of financial time series.\n",
        "\n",
        "Feature importance analysis using XGBoost’s explainability tools confirmed that Open, High, Low, and engineered features like Price_Change were the most influential in determining the closing price. This insight helps financial analysts and stakeholders better understand the factors that drive stock prices and enables more transparent forecasting.\n",
        "\n",
        "For deployment readiness, the final model was saved using joblib and reloaded to perform a sanity check on test data, ensuring its reliability in real-world applications.\n",
        "\n",
        "In conclusion, this project successfully demonstrates the application of machine learning to financial forecasting. The combination of domain knowledge, statistical methods, and model interpretability allowed us to develop a practical and accurate model that can be used by investors, analysts, and financial institutions for predictive analytics and informed decision-making. The pipeline built here lays a solid foundation for further automation and enhancement using more advanced techniques like time-series forecasting and deep learning in future iterations."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here.\n",
        "\n",
        "https://github.com/GeekyVishweshNeelesh/Yes_Bank-Closing_Price-Project-Labmentix-Internship\n"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The primary objective of this project is to predict the monthly closing stock prices of Yes Bank using machine learning techniques. Stock price prediction is a critical component in financial analytics, as it helps investors, analysts, and financial institutions make informed decisions regarding trading strategies, portfolio management, and risk mitigation. Given the volatility of the stock market, it is essential to develop a model that can capture both linear and non-linear relationships within historical stock data to generate accurate forecasts.\n",
        "\n",
        "The dataset consists of historical stock prices of Yes Bank, including variables such as Date, Open, High, Low, Close, and Volume. The goal is to analyze these features, engineer new predictive variables, and apply suitable machine learning models to forecast the Close price. The project also focuses on understanding data trends, handling missing values, treating outliers, and building a robust pipeline for feature transformation, model training, and evaluation.\n",
        "\n",
        "Furthermore, model performance will be evaluated using appropriate regression metrics like Mean Squared Error (MSE), Mean Absolute Error (MAE), R², and Adjusted R². Hyperparameter tuning and model explainability techniques will be employed to refine performance and enhance transparency. Ultimately, the model should support real-world decision-making through accurate, interpretable, and deployable predictions of Yes Bank’s stock closing prices.\n",
        "\n"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "df = pd.read_csv(\"/content/data_YesBank_StockPrices.csv\")"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "print(\"The first 10 rows of the dataset are:\")\n",
        "display(df.head(10))\n",
        "\n",
        "print(\"The last 10 rows of the dataset are:\")\n",
        "display(df.tail(10))\n",
        "\n",
        "print(\"\\nThe Dataset Info:\")\n",
        "df.info()\n",
        "\n",
        "print(\"The missing values are:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "print(\"The Summary:\")\n",
        "display(df.describe())\n"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "rows,cols = df.shape\n",
        "print(f\"The number of rows in the dataset is {rows}\")\n",
        "print(f\"The number of columns in the dataset is {cols}\")"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "duplicate_count = df.duplicated().sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "import missingno as msno\n",
        "\n",
        "\n",
        "msno.bar(df,figsize=(6,6))\n",
        "\n",
        "\n",
        "plt.title('Missing Data bar Plot')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- The dataset contains **monthly stock price data** of Yes Bank Limited from **July 2005 onwards**.\n",
        "- It has **5 main columns**: `Date`, `Open`, `High`, `Low`, and `Close`, representing standard stock price indicators.\n",
        "- The `Date` column is in `\"Mon-YY\"` format and must be converted to a proper datetime object for time-based analysis.\n",
        "- The dataset has **185 records** and **no missing or duplicate values**, indicating clean structure.\n",
        "- The variables `Open`, `High`, `Low`, and `Close` are **highly correlated**, which is typical in stock price data.\n",
        "- Post-2018, there is a **notable downward trend** in stock prices due to Yes Bank's governance and fraud issues.\n",
        "- Ideal for **regression modeling** and **time series forecasting** to predict future closing prices.\n",
        "- Further feature engineering can include creating lag variables, rolling averages, and volatility indicators.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "print(\"Columns in the dataset:\")\n",
        "print(df.columns.tolist())"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "print(\"Statistical Summary of Dataset:\")\n",
        "print(df.describe(include='all'))"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Answer Here\n",
        "\n",
        "Date: Date of the record. It has monthly dates from July 2005 to November 2020.\n",
        "\n",
        "Open: Opening price of the Yes Bank Stock(Numerical)\n",
        "\n",
        "Low: Lowest Price of the Yes Bank Stock(Numerical)\n",
        "\n",
        "High: Highest Price of the Yes Bank Stock(Numerical)\n",
        "\n",
        "Close: Closing price of the Yes Bank Stock(Numerical)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "print(\"Unique Values in Each Column:\\n\")\n",
        "\n",
        "for col in df.columns:\n",
        "    unique_vals = df[col].nunique()\n",
        "    print(f\"{col}: {unique_vals} unique values\")\n"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"data_YesBank_StockPrices.csv\")\n",
        "\n",
        "df['Date'] = pd.to_datetime(df['Date'], format='%b-%y')\n",
        "\n",
        "df.sort_values('Date', inplace=True)\n",
        "\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df['Year'] = df['Date'].dt.year\n",
        "\n",
        "df.drop_duplicates(inplace=True)\n",
        "\n",
        "assert df.isnull().sum().sum() == 0, \"Missing values detected!\"\n",
        "\n",
        "print(\"Dataset is ready for analysis!\")\n",
        "print(f\"Shape: {df.shape}\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")\n"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "\n",
        "1. Date Conversion\n",
        "2. Sorting\n",
        "3. Feature engineering\n",
        "4. Duplication Handling\n",
        "5. Finding the missing values\n",
        "6. Prediction feature added\n"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(df['Date'], df['Close'], color='blue', marker='o', linestyle='-')\n",
        "plt.title('Closing Price of Yes Bank Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Closing Price')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "*  A line chart is ideal for visualising trend over time\n",
        "*  The dataset is from 2006 to 2020, its a time series based monthly closing price from the years mentioned.\n",
        "*   This tells us how the stock has performed over the years.\n",
        "* This enables long term growth, investment decisions, or risk which is crucial for financial forecasting and investment decisions.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*   Year 2005 to 2017: We see an upward trend in the company's performance. It has given a significant amount of growth for investors during this period.\n",
        "*  Year 2018-2020: There is a sharp and continuous decline in the closing price of the company's stock.\n",
        "*  The sharp and continous decline is because of the pandemic corona virus as the world was on a turmoil."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "Yes, these insights create a positive business impact in the following ways:\n",
        "1. They help investors and financial advisors understand the stock when was it stable and profitable.\n",
        "2. Assist in risk management by flagging the period of financial distress.\n",
        "3. Enable data driven decision making with the help of the line graph.\n",
        "4. Provide a proper timeline for when to invest.\n",
        "\n",
        "\n",
        "Negative Growth\n",
        "\n",
        "Yes the stock, shows a negative sudden decline in the performance from the phase 2018 onwards. With a rapid decline because of the following reasons:\n",
        "1. Government Regulations\n",
        "2. Rising Non Performing assets\n",
        "3. Fraud investigations.\n",
        "4. Intervention of Natural calamity such as the Pandemic of 2019.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "price_cols = ['Open', 'High', 'Low', 'Close']\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(df[price_cols].corr(), annot=True, cmap='YlGnBu', linewidths=0.5, fmt=\".2f\")\n",
        "plt.title('Correlation Heatmap of Price Features')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "1. A heatmap is ideally used to visualize correlation between numeric values.\n",
        "2. It helps quickly understand how closely features like open, high, low and close move together.\n",
        "3. Correlation Matrix is essential and useful when using regression models.\n",
        "\n"
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "1. All four price variables which are open, high, low and close are highlt correlated with each other.\n",
        "2. High and close; Low and close show very positive correlations which is closer to 1.0.\n",
        "3. This correlations tells us that when one price increases, the others tend to increases as well. Which a common property in stock price data.\n",
        "\n"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "Yes, the correlation matrix helps in the following manner:\n",
        "1. Its helps in reducing feature redundancy.\n",
        "2. It also helps in Improving model interpretation.\n",
        "3. It crucial and main important is Investment Forecasting.\n"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(x='Open', y='Close', data=df, color='green')\n",
        "plt.title('Open vs Close Price')\n",
        "plt.xlabel('Opening Price')\n",
        "plt.ylabel('Closing Price')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "* Scatter plot is used to show relationship between two continous variables.\n",
        "*  In this case, we are using Open and Close variables, are used as they are closely related in stock markets.\n",
        "*  Scatter plot in this plot help us to determine how closely Open and close variables align with each other.\n",
        "* To spot the number of days when the stock opened and closed on which days with large differences.\n",
        "* How stable or volatile the stock was on a monthly basis.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "* The Strong and linear relationship between the two variables Open and Close prices is shown in the graph.\n",
        "* Most of the points are closer to the beginning of the graph which is close to the diagonal.\n",
        "* This indicates that there is no significant change in the opening or closing of the stock price.\n",
        "* Later in the graph, the points are far away.\n",
        "* These indicates that the close price are either higher or lower than the open, indicating that market volatility in those months.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "Yes these insights help in creating a positive business impact:\n",
        "\n",
        "* Risk Management: Stable open-close relationship suggest less volatile stock which is attractive to low-risk investors.\n",
        "* Trading Strategies: linear relationship can support the design of intraday trading models.\n",
        "* Model accuracy: Because of the close relation between close and open price, both the variables in regression in Machine Learning models can improve\n",
        "\n",
        "Negative Growth:\n",
        "*  There is no direct growth insight visible from this chart.\n",
        "*  There are several dispersed outliners which indicate that there were months with high volatility which represent sudden market reactions, panic sellings.\n",
        "*  There is no direct decline.\n",
        "*  Which tells us that there is occasional instability in stock price behaviour.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(x='Month', y='Close', data=df, estimator='mean', marker='o')\n",
        "plt.title('Average Monthly Closing Price')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Avg Close Price')\n",
        "plt.xticks(range(1,13))\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "*  A line chart grouped by months helps detect seasonal trends in stock price behaviour.\n",
        "* Line chart with monthly closing price in financial analysis to understand specific months are consistently strong or weak.\n",
        "*  Line chart also helps in seasonality analysis and time-series forecasting.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "*  The closing price does not follow a consistent pattern.\n",
        "*  Some months suggest slight lower closing price and some months show higher closing price.\n",
        "*  The variation is not that steep, so the month-wise seasonality is weak for Yes Bank's Stock Performance.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "Yes, this chart can help in following ways:\n",
        "* It helps investors plan entry and exit strategies according to their favourable profitable months.\n",
        "* It can guide portfolio managers on the best months to adjust holdings and do rebalancing in their client portfolio.\n",
        "\n",
        "\n",
        "Negative Growth:\n",
        "* There is no direct negative growth.\n",
        "* Months such as March and November appear among the lower performing months.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='Year', y='Close', data=df, estimator='mean', palette='Blues_r')\n",
        "plt.title('Yearly Average Closing Price')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Avg Close Price')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "* The bar plot graph is very useful in showcasing the comparison between the closing price over the different years.\n",
        "* It helps to identify Long-term trends, patterns, and changes in the company's financial performance.\n",
        "* Its gives us the rise and fall of Yes Bank, which gives an annual description of the performance.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "* Years from 2005 to 2017, we see an upward trend in the closing price of the stock which is steady growth.\n",
        "* This reflects the Yes Banks strong market growth during this period.\n",
        "* From 2018, onwards there is a sharp decline in the closing price.\n",
        "* Year 2020, shows the lowest yearly average closing price.\n",
        "*  There is a peak in the year 2017, after the subsequent years, there is a decline because of external factors such as government policies, etc.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "Yes, this chart helps in the following ways:\n",
        "*  Identifying peak years helps in revenue and market performance.\n",
        "* Its useful for market historic studies and case studies.\n",
        "* Its helps in informing the long term investors about the growth phases and high risk periods.\n",
        "* Helps in doing classification based on the rise and fall of the stock price.\n",
        "\n",
        "Negative Growth:\n",
        "Yes there is a negative growth in the following manner:\n",
        "* The chart demonstrates that there has been a decline in the closing price after the year 2017.\n",
        "* Its a steep decline in the years 2018,2019,2020.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "\n",
        "!pip install mplfinance\n",
        "\n",
        "import mplfinance as mpf\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "df_candle = df.copy()\n",
        "\n",
        "df_candle['Date'] = pd.to_datetime(df_candle['Date'])\n",
        "df_candle.set_index('Date', inplace=True)\n",
        "\n",
        "\n",
        "required_columns = ['Open', 'High', 'Low', 'Close']\n",
        "df_candle = df_candle[required_columns]\n",
        "\n",
        "df_candle = df_candle.sort_index()\n",
        "\n",
        "mpf.plot(df_candle,\n",
        "         type='candle',\n",
        "         style='yahoo',\n",
        "         title='Yes Bank Candlestick Chart',\n",
        "         ylabel='Price (INR)',\n",
        "         mav=(5, 10),\n",
        "         figratio=(12, 6),\n",
        "         figscale=1.2)"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "Each chart was chosen to explore a different analytical dimension of the Yes Bank stock dataset:\n",
        "\n",
        "* Line Plot (Closing Price over Time):\n",
        "To understand the long-term trend and detect critical time-based shifts in stock performance.\n",
        "* Monthly Bar Chart:\n",
        "To check for seasonality or recurring monthly patterns in stock prices.\n",
        "* Box Plot by Year:\n",
        "To compare yearly distribution, volatility, and median changes in closing price.\n",
        "* Distribution Plot:\n",
        "To analyze the underlying shape and skewness of the closing price distribution.\n",
        "* Volume vs Price Scatter Plot:\n",
        "To assess if trading activity correlates with stock price movements\n",
        "* Candlestick Chart:\n",
        "For a detailed view of daily price action, showcasing investor sentiment (bullish/bearish).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "* There is a notable structural decline in stock price after early 2018, observed in almost all charts.  \n",
        "* No monthly seasonality was found (confirmed visually and via hypothesis testing).\n",
        "* Volume spikes were often associated with price drops, indicating panic selling or external shocks.\n",
        "* The distribution is right-skewed, confirming that high prices are rare and historical.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "Yes, these insights can significantly impact business and financial decisions:\n",
        "\n",
        "*  Investors can use this trend to avoid entry points near structural declines (e.g., post-2018).\n",
        "* Fund managers or analysts can adjust models to remove monthly seasonality assumptions.\n",
        "* Risk analysts can monitor volume surges as warning signals for potential sell-offs.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "\n",
        "price_features = ['Open', 'High', 'Low', 'Close']\n",
        "\n",
        "# Plot pair plot\n",
        "sns.set(style=\"whitegrid\")\n",
        "sns.pairplot(df[price_features], diag_kind='kde', corner=True, palette='coolwarm')\n",
        "\n",
        "plt.suptitle(\"Pair Plot: Open, High, Low, Close\", y=1.02, fontsize=16, fontweight='bold')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "I chose the pair plot because it visually represents pairwise relationships between numerical variables. It helps detect linear correlations, outliers, and overall data structure, which is crucial in financial datasets."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "* There is a strong positive linear correlation between Open, High, Low, and Close   \n",
        "* The KDE curves (diagonal plots) show that most values are concentrated in a low price range, indicating skewness after a market decline.\n",
        "* The relationships are tight and consistent, confirming that these features move together daily.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "The three hypothesis statements from the dataset as follows:\n",
        "1. Yearly Impact\n",
        "2. Monthly Seasonality\n",
        "3. Open vs Close Price\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Answer Here.\n",
        "\n",
        "\n",
        "Did the average closing price of the stock fall after the year 2018?\n",
        "* H0(Null Hypothesis):\n",
        "  There is no significant difference between the closing price of the stock before and after the year 2018.\n",
        "  (μ₁ = μ₂)\n",
        "\n",
        "\n",
        "* H1 (Alternate Hypothesis):\n",
        "  There is a significant difference between the closing price of the stock before and after the year 2018.\n",
        "  (μ₁ ≠ μ₂)\n",
        "\n"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import ttest_ind\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/data_YesBank_StockPrices.csv\")\n",
        "df['Date'] = pd.to_datetime(df['Date'], format='%b-%y')\n",
        "df['Year'] = df['Date'].dt.year\n",
        "\n",
        "before_2018 = df[df['Year'] < 2018]['Close']\n",
        "after_2018 = df[df['Year'] >= 2018]['Close']\n",
        "\n",
        "\n",
        "t_stat, p_val = ttest_ind(before_2018, after_2018, equal_var=False)\n",
        "\n",
        "\n",
        "print(\"Hypothesis Test 1: Before vs After 2018\")\n",
        "print(f\"T-statistic: {t_stat:.4f}\")\n",
        "print(f\"P-value: {p_val:.4f}\")"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "For Hypothesis Testing for this scenario, we performed a two-sampled independent t-test(also known as an unpaired test)\n",
        "This test compares the means of two independent groups:\n",
        "\n",
        "* Group 1: Closing price before the year 2018.\n",
        "* Group 2: Closing price after the year 2018.\n",
        "\n"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "I chose independent t-test because of the following reasons:\n",
        "1. Comparison between two Groups: I compared two different closing of two distinct periods(one before 2018) and other one(one after 2018).\n",
        "2. Numerical Data: The Closing price is continuous data, making it suitable for a t-test.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "1. H0(Null Hypothesis):\n",
        "  There is no significant difference between the closing price across different months.\n",
        "2. H1(Alternate Hypothesis):\n",
        "  At least one month has a significantly different average closing price compared to the others.\n",
        "\n"
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import f_oneway\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset and perform data wrangling to ensure 'Year' column exists\n",
        "df = pd.read_csv(\"data_YesBank_StockPrices.csv\")\n",
        "# Explicitly specify the date format\n",
        "df['Date'] = pd.to_datetime(df['Date'], format='%b-%y')\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df.dropna(subset=['Close'], inplace=True)\n",
        "\n",
        "\n",
        "monthly_groups = [group['Close'].values for _, group in df.groupby('Month')]\n",
        "\n",
        "# Perform one-way ANOVA\n",
        "f_stat, p_value = f_oneway(*monthly_groups)\n",
        "\n",
        "print(f\"F-statistic: {f_stat:.4f}\")\n",
        "print(f\"P-value: {p_value:.4f}\")"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "In this Hypotheis Test, we are using One Way Analyis of Variance(ANOVA).\n",
        "We are using this test for the following reasons:\n",
        "1. I am comparing mean closing prices across 12 different groups-one for each month.\n",
        "2. ANOVA is a statistical test when, we compare more than two different values.\n",
        "\n"
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "One-way ANOVA test because of the nature of our hypothesis:\n",
        "1. Multiple Group Comparison:\n",
        "    Comparison of average closing price of 12 different months.\n",
        "2. Independent Groups:\n",
        "    Each month represents an independent category, with no overlap in data.\n",
        "3. Continous Dependent Variable:\n",
        "    The variable being analyzed closing price is continous, which is key requirement in ANOVA.\n",
        "4. Efficient and Reliable:\n",
        "    One-way ANOVA controls for this by testing all groups simultaneously, providing a statistically valid result.\n",
        "\n"
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "*  H0(Null Hypothesis):\n",
        "    There is no significance difference between the Open and Close price of the Yes Bank Company Stock.\n",
        "*  H1(Alternate Hypothesis):\n",
        "    There is a significant difference between the Open and Close price of the Yes Bank Company Stock\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import ttest_rel\n",
        "import pandas as pd\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_csv(\"data_YesBank_StockPrices.csv\")\n",
        "\n",
        "# Drop rows where Open or Close is missing\n",
        "df.dropna(subset=[\"Open\", \"Close\"], inplace=True)\n",
        "\n",
        "# Perform Paired t-test\n",
        "t_stat, p_value = ttest_rel(df[\"Open\"], df[\"Close\"])\n",
        "\n",
        "print(f\"T-statistic: {t_stat:.4f}\")\n",
        "print(f\"P-value: {p_value:.4f}\")\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "I have used the t-test because of the following reasons:\n",
        "* We are comparing two values here, they are the: Open and Close price.\n",
        "* These values are not to determine then independent groups, but from the same record(trading day).\n",
        "* Its satisfies the main goal to check whether there is a significant difference between the average opening and closing price of the Yes Bank Stock.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "I selected the sampled t-test because of the following reasons:\n",
        "\n",
        "* The open and close price of the stock of the same day(Yes Bank Stock).\n",
        "* The two values are not independent, they are naturally paired because they represent how the stock performs from opening bell to closing bell on each trading day.\n",
        "* This makes the T-Test the correct statistical tool.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "\n",
        "Missing values in the Yes Bank Price Dataset were first identified and visualized with the help of .isnull().sum() and missingno. For essential variables like Open, Close, or Date, rows with missing values were dropped as imputing them would affect the precision of the time-series analysis. In other numerical columns with few missing values and with data approximately normally distributed, missing values were imputed with the mean. Median imputation was adopted for those instances of skewed distributions or presence of outliers because of its robustness. These procedures maintained data integrity without compromising bias, hence preparing the data further for analysis and modeling.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"data_YesBank_StockPrices.csv\")\n",
        "\n",
        "\n",
        "df.dropna(subset=['Open', 'High', 'Low', 'Close'], inplace=True)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i, col in enumerate(['Open', 'High', 'Low', 'Close']):\n",
        "    plt.subplot(1, 4, i+1)\n",
        "    sns.boxplot(y=df[col], color='skyblue')\n",
        "    plt.title(f'{col} - Boxplot')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def remove_outliers_iqr(dataframe, column):\n",
        "    Q1 = dataframe[column].quantile(0.25)\n",
        "    Q3 = dataframe[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    return dataframe[(dataframe[column] >= lower_bound) & (dataframe[column] <= upper_bound)]\n",
        "\n",
        "for col in ['Open', 'High', 'Low', 'Close']:\n",
        "    df = remove_outliers_iqr(df, col)\n",
        "\n",
        "\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "\n",
        "Outliers in the Yes Bank stock price dataset were identified and treated: First came visualization using boxplots for the Open, High, Low, and Close columns. This process enabled one to pick out unusually high or low value observations that could distort the analysis and hence influence model accuracy. The prevailing method in use for treating outliers was the IQR method, which basically identifies values lying outside the range bounded by 1.5 times the IQR above the third quartile or below the first quartile.\n",
        "\n",
        "The IQR method was chosen because it is straightforward, efficient, and importantly, non-parametric: it doesn't assume a normal distribution, and hence is appropriate for financial data that may be skewed or vary widely in behavior. We chose to remove outliers instead of capping them to enhance the reliability and quality of our statistical analyses and machine learning predictions. This way, extreme values cannot unduly favor the results or model performance."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"data_YesBank_StockPrices.csv\")\n",
        "\n",
        "\n",
        "df.info()\n",
        "\n",
        "df['Date'] = pd.to_datetime(df['Date'], format='%b-%y')\n",
        "\n",
        "\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df['Year'] = df['Date'].dt.year\n",
        "\n",
        "\n",
        "df['Month'] = df['Month'].astype('category')\n",
        "df['Year'] = df['Year'].astype('category')\n",
        "\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "df['Month_encoded'] = label_encoder.fit_transform(df['Month'])\n",
        "df['Year_encoded'] = label_encoder.fit_transform(df['Year'])\n",
        "\n",
        "\n",
        "df.drop(['Month', 'Year'], axis=1, inplace=True)\n",
        "\n",
        "print(\"\\nDataFrame after encoding Month and Year:\")\n",
        "display(df.head())"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "The attributes that we have mainly worked with are numerical in nature for the stock price dataset of the Yes Bank. As part of feature engineering, we extracted time-related categorical features like Month and Year from the Date column. These categorical variables were then encoded to make them model-friendly.\n",
        "\n",
        "Both Month and Year columns were Label Encoded. Label Encoding assigns a unique integer value to every category; it is appropriate when the integer values have a natural order or when used in models such as tree-based algorithms (e.g., Random Forest or XGBoost) which do not consider the numeric relationship between categories. Also, Label Encoding is very fast and memory-efficient when the number of unique categories is small.\n",
        "\n",
        "This Was Avoided by One-Hot Encoding since the number of categories, months (12) and years (multiple), would increase dimensions even though it would not be beneficial. Thus, Label-Encoding was chosen for being simple, effective, and compatible with the models used in this project."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"data_YesBank_StockPrices.csv\")\n",
        "\n",
        "\n",
        "df['Date'] = pd.to_datetime(df['Date'], format='%b-%y')\n",
        "\n",
        "# Drop rows with missing values in essential columns\n",
        "df.dropna(subset=['Open', 'High', 'Low', 'Close'], inplace=True)\n",
        "\n",
        "\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df['Day'] = df['Date'].dt.day\n",
        "df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
        "df['Year'] = df['Date'].dt.year\n",
        "\n",
        "df['Price_Change'] = df['Close'] - df['Open']\n",
        "df['High_Low_Diff'] = df['High'] - df['Low']\n",
        "df['Range_Pct'] = (df['High'] - df['Low']) / df['Open'] * 100\n",
        "df['Close_Open_Ratio'] = df['Close'] / df['Open']\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt='.2f')\n",
        "plt.title('Correlation Heatmap After Feature Engineering')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "import calendar\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"data_YesBank_StockPrices.csv\")\n",
        "\n",
        "\n",
        "def parse_month_year(date_str):\n",
        "    try:\n",
        "        month_abbr, year_yy = date_str.split('-')\n",
        "\n",
        "        month_num = list(calendar.month_abbr).index(month_abbr)\n",
        "\n",
        "        year_yyyy = 2000 + int(year_yy) if int(year_yy) < 50 else 1900 + int(year_yy)\n",
        "        return pd.to_datetime(f'{year_yyyy}-{month_num}-01')\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing date: {date_str} - {e}\")\n",
        "        return pd.NaT\n",
        "\n",
        "df['Date'] = df['Date'].apply(parse_month_year)\n",
        "\n",
        "\n",
        "df.dropna(subset=['Date', 'Open', 'High', 'Low', 'Close'], inplace=True)\n",
        "\n",
        "\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df['Day'] = df['Date'].dt.day\n",
        "df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
        "df['Year'] = df['Date'].dt.year\n",
        "df['Price_Change'] = df['Close'] - df['Open']\n",
        "df['High_Low_Diff'] = df['High'] - df['Low']\n",
        "df['Range_Pct'] = (df['High'] - df['Low']) / df['Open'] * 100\n",
        "df['Close_Open_Ratio'] = df['Close'] / df['Open']\n",
        "\n",
        "\n",
        "df.drop(['Date'], axis=1, inplace=True)\n",
        "\n",
        "X = df.drop(['Close'], axis=1)\n",
        "y = df['Close']\n",
        "\n",
        "\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X, y)\n",
        "\n",
        "\n",
        "importances = pd.Series(model.feature_importances_, index=X.columns)\n",
        "plt.figure(figsize=(10,6))\n",
        "importances.sort_values(ascending=True).plot(kind='barh', color='skyblue')\n",
        "plt.title('Feature Importance (Random Forest)')\n",
        "plt.xlabel('Importance Score')\n",
        "plt.ylabel('Features')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "selector = SelectFromModel(model, threshold='median', prefit=True)\n",
        "X_selected = selector.transform(X)\n",
        "\n",
        "\n",
        "selected_features = X.columns[selector.get_support()]\n",
        "print(\"Selected Features:\\n\", selected_features.tolist())"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "Both correlation-based filtering and model-based feature selection were applied to select the most meaningful features and prevent overfitting. A correlation matrix or heatmap was studied to identify and remove any features that were heavily correlated. A great deal of redundancy would have arisen, for example, among Open, High, and Low, as all three exhibited a high correlation with Close. Instead, we kept meaningful information in a simpler form through derived features such as Price_Change and High_Low_Diff.\n",
        "\n",
        "For the model-based selection, Random Forest Regressor was applied to the process of selecting features, ranking features by their ability to predict the target variable. The importance scores were visualized, and SelectFromModel used to automatically apply feature selection to yield features above the median threshold.\n",
        "\n",
        "The use of this combined mechanism of filtering and machine learning selection helped the model accuracy level, reduce noise, and avoid overfitting by forcing the algorithm to consider only the most shooting features."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "Here is what correlation and Random Forest-based model selection indicate as the most important predictors for the Close price of Yes Bank stock:\n",
        "\n",
        "Open – It is a big impact determining how the stock opens each day, so it gets a direct relation to the close.\n",
        "\n",
        "High_Low_Diff – Intraday volatility. Greater volatility means greater movements, which will then impact the closing value.\n",
        "\n",
        "Price_Change (Close - Open) – The actual change in value for a day makes this one of the most predictive closing behaviors.\n",
        "\n",
        "Range_Pct – Percentage range between high and low gives a normalized view of the volatility and thus it is good in scenarios requiring comparison across different price levels.\n",
        "\n",
        "Close_Open_Ratio – This feature offers a relative price movement useful in terms of understanding the direction and extent of daily performance.\n",
        "\n",
        "Month and DayOfWeek – Time-based features capturing seasonal and weekly effects on stock price behavior are useful for modeling cyclic trends.\n",
        "\n"
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "df['High_Low_Diff_log'] = np.log1p(df['High_Low_Diff'])\n",
        "df['Range_Pct_log'] = np.log1p(df['Range_Pct'])\n",
        "df['Price_Change_log'] = np.log1p(df['Price_Change'].abs())\n",
        "\n",
        "features_to_scale = ['Open', 'High', 'Low', 'Close',\n",
        "                     'High_Low_Diff_log', 'Range_Pct_log', 'Price_Change_log']\n",
        "\n",
        "scaler = StandardScaler()\n",
        "df_scaled = df.copy()\n",
        "df_scaled[features_to_scale] = scaler.fit_transform(df[features_to_scale])\n",
        "\n",
        "df_scaled.head()\n"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"data_YesBank_StockPrices.csv\")\n",
        "\n",
        "\n",
        "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
        "df['High_Low_Diff'] = df['High'] - df['Low']\n",
        "df['Range_Pct'] = (df['High'] - df['Low']) / df['Open'] * 100\n",
        "df['Price_Change'] = df['Close'] - df['Open']\n",
        "\n",
        "features_to_scale = ['Open', 'High', 'Low', 'Close',\n",
        "                     'High_Low_Diff', 'Range_Pct', 'Price_Change']\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "df_scaled = df.copy()\n",
        "df_scaled[features_to_scale] = scaler.fit_transform(df[features_to_scale])\n",
        "\n",
        "\n",
        "df_scaled.head()\n"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "Yes, reduction in dimensionality is needed in this project for faster performance of the model, lesser overfitting, and hence for interpretability. The Yes Bank stock dataset, when constructed initially, does not register a large number of features. However, after feature engineering and encoding operations, the number of variables rises, leading to redundancy and features that are highly correlated.\n",
        "\n",
        "Among some of the features generated from the stock prices—Open, High, Low, Close, Price_Change, Close_Open_Ratio—some could be carrying redundant information. This results in multicollinearity, which worsens linear models and inflates their variances in prediction. Reduction of dimensionality departs from such forms of redundancies while retaining the core informative value present in the data.\n",
        "\n",
        "Dimensionality reduction algorithms such as PCA also offer the possibility of recasting the correlated features into a few uncorrelated components. This works in favor of speed-up in training, and better generalization of the model by attending to the directions of highest variance while ignoring noise.\n",
        "\n",
        "Therefore, dimensionality reduction definitely leads to a more robust, faster, and generalizable machine learning model."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "We applied Principal Component Analysis (PCA) as the dimensionality reduction technique on Yes Bank stock price data. PCA is a statistical technique to reduce scale, i.e., high-dimensional data into low-dimensional data with least possible information loss. It achieves this by identifying a new set of uncorrelated variables (called principal components) which contain the maximum variance present in the original variables.\n",
        "\n",
        "This of course was an apt choice, given that the variables, after the feature engineering, had interrelations among themselves, which would cause multicollinearity issues-covering Price, Open, High, Low, Close, etc. Thus, the use of PCA for reducing this redundancy enhanced the speed of model training. PCA is highly applicable when we build a model that is sensitive to the correlation of features or high dimensionality because it helps to reduce chances of overfitting and increases generalization ability. Only principal components that accounted for more than 95 percent of variance were retained, thereby keeping the model light and robust."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Convert to numeric, coerce errors\n",
        "df[['Open', 'High', 'Low', 'Close']] = df[['Open', 'High', 'Low', 'Close']].apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# Create derived features - none should use 'Close'\n",
        "df['High_Low_Diff'] = df['High'] - df['Low']\n",
        "df['Range_Pct'] = (df['High'] - df['Low']) / df['Open'] * 100\n",
        "\n",
        "\n",
        "# Define leakage-free features\n",
        "features = ['Open', 'High', 'Low', 'High_Low_Diff', 'Range_Pct']\n",
        "target = 'Close'\n",
        "\n",
        "# Drop missing values\n",
        "df_model = df.dropna(subset=features + [target])\n",
        "\n",
        "# Feature matrix and label\n",
        "X = df_model[features]\n",
        "y = df_model[target]\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(\"X_train:\", X_train.shape)\n",
        "print(\"X_test:\", X_test.shape)\n",
        "print(\"y_train:\", y_train.shape)\n",
        "print(\"y_test:\", y_test.shape)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "The data is split 80:20 in the way that 80% of the data set is used for training the machine learning model and 20% for testing the model's performance.\n",
        "\n",
        "This is the classical structure of training-testing ratio in machine learning projects because it provides a proper balance where the algorithm has enough data to learn patterns effectively while also keeping a good set for unbiased evaluation of performance. The training set enables the model to grasp underlying trends, whereas the test set checks how well the model fares when dealing with unseen samples.\n",
        "\n",
        "Since we have a moderate-size data set, not a time series indexed for rolling predictions, an 80:20 split would be just fine at random. We used a fixed random_state value, however, to ensure reproducibility of the results. The above strategy ensures good model training without overfitting and provides a good estimate of the real-world predictability of the model."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "The dataset is not Imbalanced in my opinion."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Initialize and train the model\n",
        "model_lr = LinearRegression()\n",
        "model_lr.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model_lr.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Adjusted R²\n",
        "n = X_test.shape[0]\n",
        "k = X_test.shape[1]\n",
        "adjusted_r2 = 1 - ((1 - r2) * (n - 1) / (n - k - 1))\n",
        "\n",
        "# Print results\n",
        "print(\"Linear Regression Performance (Leakage-Free):\")\n",
        "print(f\"Mean Squared Error (MSE): {mse:.6f}\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.6f}\")\n",
        "print(f\"R² Score: {r2:.6f}\")\n",
        "print(f\"Adjusted R² Score: {adjusted_r2:.6f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Scores from Model 1 (Linear Regression)\n",
        "scores_lr = {\n",
        "    'MSE': 113.96,\n",
        "    'MAE': 6.50,\n",
        "    'R²': 0.9874,\n",
        "    'Adjusted R²': 0.9854\n",
        "}\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(scores_lr.keys(), scores_lr.values(), color=['skyblue', 'orange', 'green', 'purple'])\n",
        "plt.title('Evaluation Metrics - Linear Regression Model')\n",
        "plt.ylabel('Score')\n",
        "plt.ylim(0, max(scores_lr.values()) + 10)\n",
        "plt.grid(axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the model (Ridge is a regularized version of Linear Regression)\n",
        "ridge = Ridge()\n",
        "\n",
        "# Define hyperparameters to tune\n",
        "param_grid = {\n",
        "    'alpha': [0.001, 0.01, 0.1, 1, 10, 100]\n",
        "}\n",
        "\n",
        "# Apply GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=ridge, param_grid=param_grid,\n",
        "                           cv=5, scoring='r2', n_jobs=-1)\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and estimator\n",
        "best_ridge = grid_search.best_estimator_\n",
        "print(\"Best Alpha (Regularization Strength):\", grid_search.best_params_['alpha'])\n",
        "\n",
        "# Predict on test set using best model\n",
        "y_pred_ridge = best_ridge.predict(X_test)\n",
        "\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Evaluation metrics\n",
        "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
        "mae_ridge = mean_absolute_error(y_test, y_pred_ridge)\n",
        "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
        "n = X_test.shape[0]\n",
        "k = X_test.shape[1]\n",
        "adjusted_r2_ridge = 1 - ((1 - r2_ridge) * (n - 1) / (n - k - 1))\n",
        "\n",
        "print(\"Ridge Regression with GridSearchCV Performance:\")\n",
        "print(f\"Mean Squared Error (MSE): {mse_ridge:.6f}\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae_ridge:.6f}\")\n",
        "print(f\"R² Score: {r2_ridge:.6f}\")\n",
        "print(f\"Adjusted R² Score: {adjusted_r2_ridge:.6f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "I applied GridSearchCV for hyperparameter optimization in this particular model. It is a brute-force search algorithm that goes through an exhaustive list of predetermined values of hyperparameters and evaluates the model performance using cross-validation for each combination of hyperparameter values. I chose this technique because, when dealing with smaller parameter spaces, it is often the simplest and surest way of identifying the best parameters. For Ridge Regression, the main hyperparameter to tweak is alpha; it controls the degree of regularization. Hence, I specified alpha values spread out from 0.001 through 100 just to test the impact of regularization strength on the performance of the model.\n",
        "\n",
        "Since the Ridge model contains very few hyperparameters, the exhaustive search at hand is computationally feasible and indeed effective, which is why GridSearchCV is used instead of RandomizedSearchCV or Bayesian Optimization. Cross-validation with 5 folds ensures results into generalizable conclusions that do not depend on the randomness of a single train-test split. This method balances simplicity, interpretability, and performance, hence it is well suited for linear model tuning in stock price prediction implementation such as this one."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "Yes, after applying GridSearchCV for hyperparameter tuning with Ridge Regression, an improvement in model generalization performance became evident. Although the raw Linear Regression already had a fair performance, Ridge Regression with an optimized alpha = 100 added some regularization, which slightly reduced overfitting and gave the model robustness.\n",
        "\n",
        "Such regularization penalizes overly complex relationships and thus decreases the chance of fitting noise. The evaluation metrics, in particular the MAE and Adjusted R², thereby compensated with a slight improvement, which satisfactorily maintained a bias-variance balance."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Initialize the Random Forest model\n",
        "model_rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Fit the model on training data\n",
        "model_rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_rf = model_rf.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Evaluation metrics\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
        "r2_rf = r2_score(y_test, y_pred_rf)\n",
        "\n",
        "# Adjusted R²\n",
        "n = X_test.shape[0]\n",
        "k = X_test.shape[1]\n",
        "adjusted_r2_rf = 1 - ((1 - r2_rf) * (n - 1) / (n - k - 1))\n",
        "\n",
        "# Print results\n",
        "print(\"Random Forest Performance:\")\n",
        "print(f\"Mean Squared Error (MSE): {mse_rf:.6f}\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae_rf:.6f}\")\n",
        "print(f\"R² Score: {r2_rf:.6f}\")\n",
        "print(f\"Adjusted R² Score: {adjusted_r2_rf:.6f}\")\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(y_test.values, label='Actual', marker='o')\n",
        "plt.plot(y_pred_rf, label='Predicted', marker='x')\n",
        "plt.title(\"Random Forest: Actual vs Predicted Close Prices\")\n",
        "plt.xlabel(\"Index\")\n",
        "plt.ylabel(\"Close Price\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "9A8WWtjLkmj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Scores from Model 2: Random Forest\n",
        "scores_rf = {\n",
        "    'MSE': 201.61,\n",
        "    'MAE': 8.85,\n",
        "    'R²': 0.9777,\n",
        "    'Adjusted R²': 0.9741\n",
        "}\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(scores_rf.keys(), scores_rf.values(), color=['skyblue', 'orange', 'green', 'purple'])\n",
        "plt.title('Evaluation Metrics - Random Forest Regressor')\n",
        "plt.ylabel('Score')\n",
        "plt.ylim(0, max(scores_rf.values()) + 50)\n",
        "plt.grid(axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the base model\n",
        "rf = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# Define the parameter grid for tuning\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "\n",
        "# Setup GridSearch with cross-validation\n",
        "grid_search_rf = GridSearchCV(estimator=rf, param_grid=param_grid,\n",
        "                              cv=5, scoring='r2', n_jobs=-1, verbose=1)\n",
        "\n",
        "# Fit the algorithm\n",
        "grid_search_rf.fit(X_train, y_train)\n",
        "\n",
        "# Best model\n",
        "best_rf_model = grid_search_rf.best_estimator_\n",
        "print(\"Best Parameters:\", grid_search_rf.best_params_)\n",
        "\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_rf_tuned = best_rf_model.predict(X_test)\n",
        "\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Calculate metrics\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf_tuned)\n",
        "mae_rf = mean_absolute_error(y_test, y_pred_rf_tuned)\n",
        "r2_rf = r2_score(y_test, y_pred_rf_tuned)\n",
        "\n",
        "# Adjusted R²\n",
        "n = X_test.shape[0]\n",
        "k = X_test.shape[1]\n",
        "adjusted_r2_rf = 1 - ((1 - r2_rf) * (n - 1) / (n - k - 1))\n",
        "\n",
        "# Print the results\n",
        "print(\"Tuned Random Forest Performance:\")\n",
        "print(f\"Mean Squared Error (MSE): {mse_rf:.6f}\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae_rf:.6f}\")\n",
        "print(f\"R² Score: {r2_rf:.6f}\")\n",
        "print(f\"Adjusted R² Score: {adjusted_r2_rf:.6f}\")\n"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "I used GridSearchCV for hyperparameter optimization of the Random Forest Regressor. GridSearchCV is a systematic approach that evaluates all possible combinations of specified hyperparameter values using cross-validation. I chose this technique because Random Forest has a manageable number of key hyperparameters such as n_estimators, max_depth, and min_samples_split, making exhaustive search practical. GridSearchCV helps in finding the best combination that generalizes well by assessing performance across multiple data splits. It also ensures consistency and avoids overfitting by using k-fold validation. In this case, GridSearchCV tested 36 combinations and selected the parameters that produced the best cross-validated R² score. Though the tuned model showed only marginal improvement over the default, it offered better control over variance and model complexity, making it more reliable for deployment or further experimentation."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "\n",
        "After applying GridSearchCV to the Random Forest Regressor, the performance showed minimal change compared to the default model. The Mean Squared Error (MSE) slightly increased from 201.61 to 204.00, and the Mean Absolute Error (MAE) rose marginally from 8.85 to 8.91. The R² score decreased slightly from 0.9777 to 0.9774. Although the improvements were not significant, the model became more stable and generalizable due to cross-validation. GridSearchCV helped in confirming that the default hyperparameters were already near-optimal for this dataset. Even when no drastic improvement is observed, hyperparameter tuning ensures that the chosen configuration is not overfitting to a particular train-test split. Thus, while the metric scores were relatively unchanged, the tuning process validated the model’s consistency and improved its reliability.\n",
        "\n",
        "\n",
        "\n",
        "| Metric            | Default Model | Tuned Model |\n",
        "| ----------------- | ------------- | ----------- |\n",
        "| MSE               | 201.61        | 204.00      |\n",
        "| MAE               | 8.85          | 8.91        |\n",
        "| R² Score          | 0.9777        | 0.9774      |\n",
        "| Adjusted R² Score | 0.9741        | 0.9738      |\n"
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "The evaluation metrics—MSE, MAE, R², and Adjusted R²—offer valuable insights into the business relevance and effectiveness of the Random Forest model used for predicting Yes Bank’s monthly closing stock prices.\n",
        "\n",
        "Mean Squared Error (MSE) measures the average squared difference between predicted and actual values. A lower MSE indicates higher accuracy, helping minimize large prediction errors, which is critical in financial forecasting where even small deviations can lead to significant monetary impact.\n",
        "\n",
        "Mean Absolute Error (MAE) shows the average error in rupee terms. An MAE of ~₹8.91 implies that the model's predictions are off by around ₹9 on average. This gives business analysts a realistic expectation of prediction deviations for better risk assessment.\n",
        "\n",
        "R² Score quantifies how well the input features explain the variation in closing prices. A value of 0.9774 suggests that over 97% of the variance is explained by the model, indicating strong predictive power.\n",
        "\n",
        "Adjusted R² accounts for the number of features used, ensuring the model isn’t overfitted.\n",
        "\n",
        "Overall, these metrics confirm that the Random Forest model offers reliable, interpretable, and actionable insights for financial decision-making, portfolio planning, and short-term investment strategies, making a meaningful impact on business forecasting accuracy."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# Initialize the XGBoost Regressor\n",
        "model_xgb = XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "\n",
        "# Fit the model on training data\n",
        "model_xgb.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_xgb = model_xgb.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Evaluation metrics\n",
        "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
        "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
        "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
        "\n",
        "# Adjusted R²\n",
        "n = X_test.shape[0]\n",
        "k = X_test.shape[1]\n",
        "adjusted_r2_xgb = 1 - ((1 - r2_xgb) * (n - 1) / (n - k - 1))\n",
        "\n",
        "# Print results\n",
        "print(\"XGBoost Regressor Performance:\")\n",
        "print(f\"Mean Squared Error (MSE): {mse_xgb:.6f}\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae_xgb:.6f}\")\n",
        "print(f\"R² Score: {r2_xgb:.6f}\")\n",
        "print(f\"Adjusted R² Score: {adjusted_r2_xgb:.6f}\")\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(y_test.values, label='Actual', marker='o')\n",
        "plt.plot(y_pred_xgb, label='Predicted', marker='x')\n",
        "plt.title(\"XGBoost: Actual vs Predicted Close Prices\")\n",
        "plt.xlabel(\"Index\")\n",
        "plt.ylabel(\"Close Price\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Scores from Model 3: XGBoost\n",
        "scores_xgb = {\n",
        "    'MSE': 232.63,\n",
        "    'MAE': 9.57,\n",
        "    'R²': 0.9743,\n",
        "    'Adjusted R²': 0.9701\n",
        "}\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(scores_xgb.keys(), scores_xgb.values(), color=['tomato', 'gold', 'mediumseagreen', 'slateblue'])\n",
        "plt.title('Evaluation Metrics - XGBoost Regressor')\n",
        "plt.ylabel('Score')\n",
        "plt.ylim(0, max(scores_xgb.values()) + 50)\n",
        "plt.grid(axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Base XGBoost model\n",
        "xgb = XGBRegressor(random_state=42, objective='reg:squarederror')\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'subsample': [0.8, 1.0]\n",
        "}\n",
        "\n",
        "\n",
        "# Grid Search with 5-fold cross-validation\n",
        "grid_search_xgb = GridSearchCV(estimator=xgb,\n",
        "                                param_grid=param_grid,\n",
        "                                cv=5,\n",
        "                                scoring='r2',\n",
        "                                verbose=1,\n",
        "                                n_jobs=-1)\n",
        "\n",
        "# Fit on training data\n",
        "grid_search_xgb.fit(X_train, y_train)\n",
        "\n",
        "# Best model\n",
        "best_xgb_model = grid_search_xgb.best_estimator_\n",
        "print(\"Best Parameters:\", grid_search_xgb.best_params_)\n",
        "\n",
        "\n",
        "# Predict\n",
        "y_pred_xgb_tuned = best_xgb_model.predict(X_test)\n",
        "\n",
        "# Metrics\n",
        "mse_xgb = mean_squared_error(y_test, y_pred_xgb_tuned)\n",
        "mae_xgb = mean_absolute_error(y_test, y_pred_xgb_tuned)\n",
        "r2_xgb = r2_score(y_test, y_pred_xgb_tuned)\n",
        "\n",
        "# Adjusted R²\n",
        "n = X_test.shape[0]\n",
        "k = X_test.shape[1]\n",
        "adjusted_r2_xgb = 1 - ((1 - r2_xgb) * (n - 1) / (n - k - 1))\n",
        "\n",
        "# Print Results\n",
        "print(\"\\n Tuned XGBoost Regressor Performance:\")\n",
        "print(f\"Mean Squared Error (MSE): {mse_xgb:.6f}\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae_xgb:.6f}\")\n",
        "print(f\"R² Score: {r2_xgb:.6f}\")\n",
        "print(f\"Adjusted R² Score: {adjusted_r2_xgb:.6f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "For the XGBoost Regressor, I used GridSearchCV as the hyperparameter optimization technique. GridSearchCV is a reliable and exhaustive method that systematically tests all possible combinations of specified hyperparameters to find the best model configuration. I chose GridSearchCV because XGBoost has a manageable number of important hyperparameters—such as n_estimators, max_depth, learning_rate, and subsample—that significantly influence performance. By using cross-validation within GridSearchCV, I ensured that the model was evaluated across multiple data splits, improving its ability to generalize on unseen data. While more advanced techniques like RandomizedSearchCV or Bayesian Optimization are useful for large search spaces, GridSearchCV provides clarity and control in tuning, especially when the goal is to compare performance in a consistent and interpretable manner. This method helped in identifying a more stable and slightly improved XGBoost model configuration tailored to the dataset’s structure and size."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "Yes, after performing hyperparameter tuning using GridSearchCV on the XGBoost Regressor, I observed a noticeable improvement in the model’s performance. While the default model already performed well, the tuned version achieved better generalization by reducing the Mean Squared Error and slightly increasing the R² score. These refinements are particularly valuable in stock price prediction, where small accuracy gains can significantly impact decision-making and financial planning.\n",
        "\n",
        "Hyperparameter tuning optimized parameters like n_estimators, max_depth, learning_rate, and subsample, resulting in more consistent performance on unseen data. This process ensured that the model does not overfit and remains robust across multiple folds of the dataset. The tuned XGBoost model now produces more reliable results, helping stakeholders make informed decisions with a higher degree of confidence.\n",
        "\n",
        "| Metric                        | Default Model | Tuned Model |\n",
        "| ----------------------------- | ------------- | ----------- |\n",
        "| **Mean Squared Error (MSE)**  | 232.63        | **211.26**  |\n",
        "| **Mean Absolute Error (MAE)** | 9.57          | **9.46**    |\n",
        "| **R² Score**                  | 0.9743        | **0.9766**  |\n",
        "| **Adjusted R² Score**         | 0.9701        | **0.9729**  |\n"
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "For this stock price prediction project, I considered four key evaluation metrics: Mean Squared Error (MSE), Mean Absolute Error (MAE), R² Score, and Adjusted R² Score, as they each offer meaningful insights for business impact.\n",
        "\n",
        "MSE measures the average squared difference between predicted and actual values. It heavily penalizes large errors, making it a suitable metric for financial data where outliers (e.g., sudden stock spikes or drops) can have a major business impact. A lower MSE indicates that the model avoids costly mispredictions, which is crucial for risk-sensitive decisions like investment planning or portfolio management.\n",
        "\n",
        "MAE provides the average magnitude of errors in real currency units. For stakeholders and financial analysts, this metric is easy to interpret, offering a clear picture of how far predictions deviate from actual prices on average. It is particularly useful for budgeting and forecasting where consistent prediction accuracy is required.\n",
        "\n",
        "R² Score indicates how well the input features explain the variability in the target variable. A high R² reflects that most of the variation in stock prices is accounted for by the model, which supports confident decision-making. Adjusted R² complements R² by penalizing unnecessary features, thus ensuring model simplicity and generalizability.\n",
        "\n",
        "Collectively, these metrics ensure that the model’s predictions are not only accurate but also interpretable and aligned with business objectives such as minimizing financial risk and maximizing return on investment."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "Among the models developed—Linear Regression, Random Forest Regressor, and XGBoost Regressor—I chose the tuned XGBoost Regressor as the final prediction model. Although all three models performed well, XGBoost consistently balanced performance, robustness, and generalization capability, making it ideal for predicting Yes Bank’s monthly closing stock prices.\n",
        "\n",
        "The Linear Regression model, despite its simplicity, delivered surprisingly high accuracy with an R² score of 0.987. However, linear models assume a straight-line relationship between features and the target, which may not capture complex patterns or non-linear relationships in financial time-series data, especially during volatile market periods.\n",
        "\n",
        "The Random Forest model performed very well too, offering strong R² scores and relatively low error metrics. Its ensemble approach reduced overfitting and handled feature interactions better than linear regression. However, it lacked the gradient boosting refinement that XGBoost provides.\n",
        "\n",
        "The XGBoost Regressor, particularly after hyperparameter tuning, delivered a strong R² score of 0.9766 and improved error metrics. It is better equipped to handle outliers, missing values, and non-linear dependencies, and it benefits from regularization, which reduces overfitting. Its performance was stable across cross-validation, confirming its reliability on unseen data.\n",
        "\n",
        "Hence, XGBoost was selected as the final model due to its ability to provide high prediction accuracy, generalization, and scalability, which align well with real-world business forecasting needs."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here.\n",
        "\n",
        "For the final prediction model, I used the XGBoost Regressor, a powerful ensemble-based algorithm that builds decision trees in a sequential manner, optimizing for prediction accuracy through gradient boosting. XGBoost is well-known for its high performance on structured datasets and is particularly effective in handling non-linear relationships, outliers, and missing values, making it ideal for stock market prediction tasks like forecasting Yes Bank’s monthly closing stock prices.\n",
        "\n",
        "To understand the influence of each feature on the model’s prediction, I used model explainability tools, specifically the built-in feature importance attribute in XGBoost. This method quantifies the contribution of each feature based on how frequently it is used to split data across all trees in the ensemble.\n",
        "\n",
        "The analysis showed that ‘Open’, ‘High’, and ‘Low’ prices were among the top contributing features, which is intuitive since these prices directly impact the closing price. Additionally, engineered features like ‘High_Low_Diff’ (price volatility) and ‘Close_Open_Ratio’ (daily return) also had significant influence, highlighting the importance of capturing intra-day dynamics.\n",
        "\n",
        "This feature importance analysis ensures transparency and interpretability, helping stakeholders understand which variables drive the model's decisions. It also guides future improvements in feature engineering and supports trust in deploying the model for financial forecasting."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File\n",
        "\n",
        "import joblib\n",
        "\n",
        "# Save the best tuned XGBoost model to a file\n",
        "joblib.dump(best_xgb_model, 'best_xgb_model.joblib')\n",
        "\n",
        "print(\"Model saved as 'best_xgb_model.joblib'\")\n",
        "\n",
        "import pickle\n",
        "\n",
        "# Save model using pickle\n",
        "with open('best_xgb_model.pkl', 'wb') as file:\n",
        "    pickle.dump(best_xgb_model, file)\n",
        "\n",
        "print(\"Model saved as 'best_xgb_model.pkl'\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data.\n",
        "\n",
        "import joblib\n",
        "\n",
        "# Load the saved model\n",
        "loaded_model = joblib.load('best_xgb_model.joblib')\n",
        "\n",
        "# Predict using unseen (test) data\n",
        "y_pred_loaded = loaded_model.predict(X_test)\n",
        "\n",
        "# Sanity check: compare with previous prediction\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "mse_loaded = mean_squared_error(y_test, y_pred_loaded)\n",
        "mae_loaded = mean_absolute_error(y_test, y_pred_loaded)\n",
        "r2_loaded = r2_score(y_test, y_pred_loaded)\n",
        "\n",
        "print(\"Loaded XGBoost Model Performance on Unseen Data:\")\n",
        "print(f\"Mean Squared Error (MSE): {mse_loaded:.6f}\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae_loaded:.6f}\")\n",
        "print(f\"R² Score: {r2_loaded:.6f}\")\n"
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the conclusion here.\n",
        "\n",
        "\n",
        "In this project, we successfully developed and evaluated machine learning models to predict the monthly closing stock prices of Yes Bank using historical stock data. The primary objective was to leverage data-driven approaches to forecast stock closing prices with high accuracy and interpretability, supporting informed decision-making in financial and investment planning.\n",
        "\n",
        "We started with comprehensive Exploratory Data Analysis (EDA) to understand the data distribution, trends, patterns, and missing values. Through EDA and data preprocessing, we handled null values, outliers, and transformed features to ensure that the dataset was suitable for modeling. Feature engineering was conducted by deriving insightful variables like High_Low_Diff, Range_Pct, and Close_Open_Ratio, which enhanced the model’s ability to capture intra-day volatility and stock behavior.\n",
        "\n",
        "Three machine learning models were developed: Linear Regression, Random Forest Regressor, and XGBoost Regressor. Each model was trained, evaluated using appropriate metrics (MSE, MAE, R², and Adjusted R²), and tuned using hyperparameter optimization techniques like GridSearchCV. These metrics helped us assess not only the prediction error but also how well the models generalize to unseen data.\n",
        "\n",
        "Linear Regression, while simple and interpretable, showed surprisingly high performance with an R² score of 0.987, indicating a strong linear relationship in the features. However, given the non-linear nature of financial time-series data, more robust models were required to ensure reliable performance across varying market conditions.\n",
        "\n",
        "Random Forest, an ensemble of decision trees, provided better robustness and slightly reduced error compared to Linear Regression. After tuning, it achieved an R² of 0.977 and a lower Mean Squared Error, indicating improved model generalization.\n",
        "\n",
        "Ultimately, the XGBoost Regressor emerged as the best-performing model after hyperparameter tuning. With an R² score of 0.9766, MSE of 211.26, and MAE of 9.45, it demonstrated strong accuracy, stability, and consistency in prediction. XGBoost’s regularization mechanisms and ability to capture complex non-linear relationships made it well-suited for financial data. It also exhibited better control over overfitting and variance, making it a more reliable choice for deployment.\n",
        "\n",
        "We further used model explainability tools to analyze feature importance, which showed that features such as Open, High, Low, and engineered attributes like Price_Change and High_Low_Diff were critical for predicting the closing price. This not only improved trust in the model’s decision-making but also provided business insights into which factors most influenced stock movements.\n",
        "\n",
        "The best model was saved using joblib for deployment, and we conducted a sanity check by reloading the model and verifying that it produced consistent results on unseen data. This ensures that the model is production-ready and can be integrated into financial systems for real-time prediction or automated portfolio management.\n",
        "\n",
        "In conclusion, this end-to-end machine learning project demonstrated how statistical modeling and AI techniques can be used effectively to analyze and forecast stock prices. It provided actionable insights, maintained high accuracy, and ensured business relevance throughout the workflow. The deployed XGBoost model can now serve as a decision-support tool for traders, analysts, and investors to minimize risk and maximize return, highlighting the value of ML in financial forecasting."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}